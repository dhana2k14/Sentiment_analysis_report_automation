{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment ABSA with Azure AKS**\n",
    "\n",
    "- This notebooks explains how to deploy a trained model as REST API in Azure Kubernetes Cluster (AKS)\n",
    "- Once deployed the REST API can be used to get prediction (real time and/or batch scoring is supported)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Azure Core Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with proxy (optional)\n",
    "# import os \n",
    "# proxy = \"https://<username>:<password>@<proxy-ip>:<port>\"\n",
    "# os.environ['http_proxy'] = proxy\n",
    "# os.environ['https_proxy'] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize AML Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = Workspace.from_config('../../config/config.json')\n",
    "    print(ws.name, ws.location, ws.resource_group, sep = '\\t')\n",
    "    print('Initializing Workspace succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Register Model**\n",
    "- We use Inference fuction of ABSA for prediction\n",
    "- The Inference function requires two files to operate,  namely aspect and opinion lexicons. These are obtained as output from Model Training phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register the model\n",
    "# Load Aspect file\n",
    "aspect_lex = Model.register(model_path = \"../../input/generated_aspect_lex_updated_v3.csv\", # model folder\n",
    "                       model_name = \"c_aspect_lex\", #model name\n",
    "                       workspace = ws)\n",
    "print(aspect_lex.name, aspect_lex.version, sep = '\\n')\n",
    "\n",
    "# Load Opinion file\n",
    "opinion_lex = Model.register(model_path = \"../../input/generated_opinion_lex_reranked_v3.csv\", # model folder\n",
    "                       model_name = \"c_opinion_lex\", #model name\n",
    "                       workspace = ws)\n",
    "print(opinion_lex.name, opinion_lex.version, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an Environment**\n",
    "\n",
    "All model dependencies are installed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an environment\n",
    "myenv = Environment(name = \"absa-env\")\n",
    "myenv.python.conda_dependencies = CondaDependencies.create(pip_packages = [\"azureml-defaults\", \"azureml-monitoring\", \"nlp_architect\"], conda_packages=[\"nltk\"])\n",
    "myenv.environment_variables={'NLP_ARCHITECT_BE': 'CPU'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entry Script**\n",
    "\n",
    "This is a script that executes to get prediction from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile 'scoring.py'\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from nltk import flatten\n",
    "from azureml.core.model import Model\n",
    "from pathlib import Path\n",
    "from os import path\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "# Import NLP architect\n",
    "from nlp_architect.models.absa.inference.inference import SentimentInference\n",
    "from spacy.cli.download import download as spacy_download\n",
    "\n",
    "# Load language model\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)\n",
    "\n",
    "def init():\n",
    "    global inference\n",
    "    spacy_download('en')\n",
    "    aspect_lex_path = Model.get_model_path('c_aspect_lex')\n",
    "    opinion_lex_path = Model.get_model_path('c_opinion_lex') \n",
    "    print(\"%------------------------------------------%\")\n",
    "    print(\"aspect_lex_path: \", Path(aspect_lex_path))\n",
    "    print(\"current wd: \", os.getcwd())\n",
    "    path = Path(aspect_lex_path)\n",
    "    print(\"pathlib-exists()---->\",path.exists())\n",
    "    print(\"Path :\", path)\n",
    "    print(\"Parent :\", Path(aspect_lex_path).parent.parent.parent)\n",
    "    print(os.listdir(Path(aspect_lex_path).parent.parent.parent))\n",
    "    print(\"%-----------------------------------------%\")\n",
    "    inference = SentimentInference(aspect_lex_path, opinion_lex_path)\n",
    "    \n",
    "def run(raw_data):\n",
    "    try:\n",
    "        line_1 = json.loads(raw_data)['name']\n",
    "        line_2 = json.loads(raw_data)['news']\n",
    "        sentiment_doc = inference.run(doc = line_2)\n",
    "        if sentiment_doc != None:\n",
    "            labels = doc2label(sentiment_doc)\n",
    "            labels['_vendor_name'] = line_1\n",
    "            labels = labels_enhancer(labels)\n",
    "            return labels        \n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n",
    "    \n",
    "# Custom functions\n",
    "def word_freq(word_list):\n",
    "    \"\"\"\n",
    "    Return Polarity    \n",
    "    \"\"\"\n",
    "    word_freq = [word_list.count(w) for w in word_list]\n",
    "    return(dict(zip(word_list, word_freq)))\n",
    "\n",
    "def doc2label(doc):\n",
    "    \"\"\"\n",
    "    Converts ABSA Inference Doc to Sentiment Labels  \n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    sentences_list = []\n",
    "    line_json = json.loads(doc.json())\n",
    "    text = line_json['_doc_text']\n",
    "    doc = nlp(text)\n",
    "    num_sents = len(list(doc.sents))\n",
    "    sents = line_json['_sentences']\n",
    "    events = []\n",
    "    for i in range(len(sents)):\n",
    "        for e in sents[i]['_events']:\n",
    "            for ev in e:\n",
    "                if ev['_type'] == 'OPINION':\n",
    "                     events.append(ev)\n",
    "    events = {d['_text']:d for d in events}.values() # get unique events\n",
    "    tokens = text.split()\n",
    "    io = [[re.sub(r'(\\,)|(\\.)|(\\')|(\\))|(\\()|(\\!)|(\\\")', '', token), 'O'] for token in tokens] # remove punctuation from token terms\n",
    "    index = 0\n",
    "    for token_id, token in enumerate(tokens):\n",
    "        for event in events:\n",
    "            if event['_start'] == index:\n",
    "                io[token_id][1] = \"<{}>\".format(event['_polarity'])\n",
    "        index += len(token) + 1\n",
    "    io = flatten(io)\n",
    "    while 'O' in io:\n",
    "        io.remove('O')\n",
    "    output = \" \".join([l for l in io])\n",
    "    # collect opinion terms for review\n",
    "    for id, sent in enumerate(sents):\n",
    "        sent_polarity = {}\n",
    "        s = text[sent['_start']:sent['_end'] + 1]\n",
    "        s_tokens = s.split()\n",
    "        s_io = [[re.sub(r'(\\,)|(\\.)|(\\')|(\\))|(\\()|(\\!)|(\\\")', '', tok), 'O'] for tok in s_tokens]\n",
    "        terms_dict = {}\n",
    "        pos_terms_list, neg_terms_list = [], []\n",
    "        for tok_id, tok in enumerate(s_tokens):\n",
    "            for event in events:\n",
    "                if event['_text'] == re.sub(r'(\\,)|(\\.)|(\\')|(\\))|(\\()|(\\!)|(\\\")', '', tok):\n",
    "                    s_io[tok_id][1] = \"<{}>\".format(event['_polarity'])\n",
    "                    if event['_polarity'] == 'POS':\n",
    "                        if event['_polarity'] in terms_dict:\n",
    "                            pos_terms_list.append(event['_text'])\n",
    "                            terms_dict[event['_polarity']] = pos_terms_list\n",
    "                        else:\n",
    "                            pos_terms_list.append(event['_text'])\n",
    "                            terms_dict[event['_polarity']] = pos_terms_list\n",
    "                    else:\n",
    "                        if event['_polarity'] in terms_dict:\n",
    "                            neg_terms_list.append(event['_text'])\n",
    "                        else:\n",
    "                            neg_terms_list.append(event['_text'])\n",
    "                            terms_dict[event['_polarity']] = neg_terms_list\n",
    "        s_io = flatten(s_io)\n",
    "        while 'O' in s_io:\n",
    "            s_io.remove('O')\n",
    "        sentence = \" \".join([l for l in s_io])\n",
    "        sent_polarity['sentence ' + str(id + 1)] = sentence # sentence dict\n",
    "        sent_polarity['_opinion_terms'] = terms_dict\n",
    "        sentences_list.append(sent_polarity)\n",
    "    documents['_news_text'] = text\n",
    "    documents['_sentences'] = sentences_list\n",
    "    documents['#sents_actual'] = num_sents\n",
    "    documents['#sents_model'] = len(sents)\n",
    "    documents['#sents_no_model'] = num_sents - len(sents)\n",
    "    return documents\n",
    "\n",
    "def labels_enhancer(documents):\n",
    "    scores = {}\n",
    "    name = re.sub(r'(\\,)|(\\.)|(\\))|(\\()', '', documents['_vendor_name'])\n",
    "    keyword = name.lower().split()\n",
    "    keyword_short = ''.join([word[0] for word in keyword])\n",
    "    keyword = ' '.join([word for word in keyword])\n",
    "    keyword_split = keyword.split()\n",
    "    regex = re.compile(r'\\b(?:%s)' %  '|'.join(flatten([keyword_short, keyword_split, keyword])))\n",
    "    lookup_index_wt_name, lookup_index_wo_name = [], []\n",
    "    # Lookup vendor name string in sentences and calculate polarity\n",
    "    for sent in documents['_sentences']:\n",
    "        polarity = ''\n",
    "        lookup = [1 if re.search(regex, str(v).lower()) else 0 for k, v in sent.items()][0]\n",
    "        kv_pair = {'_vendor_name':lookup}\n",
    "        sent.update(kv_pair)\n",
    "        # re-calculate sentences polarity for tie cases\n",
    "        sent_polarity = re.findall(r'<NEG>|<POS>', [v for k, v in sent.items()][0])\n",
    "        _sentence = [v for k, v in sent.items()][0]\n",
    "        _vendor_name = [v for k, v in sent.items()][1]\n",
    "        sent_polarity = [re.sub(r'<|>', '', i) for i in sent_polarity]\n",
    "        if sent_polarity:\n",
    "            # polarity = ''\n",
    "            pol_list = word_freq(sent_polarity)\n",
    "            _pos = max([v if k == 'POS' else 0 for k, v in pol_list.items()])\n",
    "            _neg = max([v if k == 'NEG' else 0 for k, v in pol_list.items()])\n",
    "            if _pos == _neg:\n",
    "                words_cnt = len(_sentence.split(' '))\n",
    "                if _vendor_name == 1:\n",
    "                    pos_score = (1 / words_cnt) * 1 * 1\n",
    "                    neg_score = (1 / words_cnt) * 1 * 1.5 \n",
    "                    if pos_score < neg_score:polarity = 'Negative'\n",
    "                    else:polarity = 'Positive'\n",
    "                else:\n",
    "                    pos_score = (1 / words_cnt) * 1 \n",
    "                    neg_score = (1 / words_cnt) * 1.5 \n",
    "                    if pos_score < neg_score:polarity = 'Negative'\n",
    "                    else:polarity = 'Positive'                     \n",
    "            else:\n",
    "                if max(word_freq(sent_polarity), key = word_freq(sent_polarity).get) == 'POS':polarity = 'Positive'\n",
    "                else:polarity = 'Negative'\n",
    "        kv_pair_2 = {'polarity': polarity}\n",
    "        sent.update(kv_pair_2)\n",
    "\n",
    "        if lookup == 1:\n",
    "            sent_pol = [v if k == 'polarity' else None for k, v in sent.items()]\n",
    "            lookup_index_wt_name.append(sent_pol)\n",
    "        else:\n",
    "            sent_pol = [v if k == 'polarity' else None for k, v in sent.items()]\n",
    "            lookup_index_wo_name.append(sent_pol)\n",
    "    lookup_index_wt_name = flatten(lookup_index_wt_name)\n",
    "    lookup_index_wo_name = flatten(lookup_index_wo_name)\n",
    "    while None in lookup_index_wt_name:\n",
    "        lookup_index_wt_name.remove(None)\n",
    "    while None in lookup_index_wo_name:\n",
    "        lookup_index_wo_name.remove(None)\n",
    "\n",
    "    # Count no of pos and neg sentences in a given news text\n",
    "    sent_pol_list = []\n",
    "    for sentences in documents['_sentences']:\n",
    "        sent_pol_list.append([v if k == 'polarity' else None for k, v in sentences.items()])   \n",
    "    sent_pol_count = word_freq(flatten(sent_pol_list))\n",
    "    del sent_pol_count[None] # remove None Key from dict\n",
    "    \n",
    "    # Calculate final scores\n",
    "    documents['#neg_sents'] = max([v if k == 'Negative' else 0 for k, v in sent_pol_count.items()])\n",
    "    documents['#pos_sents'] = max([v if k == 'Positive' else 0 for k, v in sent_pol_count.items()])\n",
    "    documents['#pol_sents_wt_name'] = word_freq(lookup_index_wt_name)\n",
    "    documents['#pol_sents_wo_name'] = word_freq(lookup_index_wo_name)\n",
    "    if documents['#pol_sents_wt_name']:\n",
    "        pos_sents_wt_name = max([v if k == 'Positive' else 0 for k, v in documents['#pol_sents_wt_name'].items()])\n",
    "        neg_sents_wt_name = max([v if k == 'Negative' else 0 for k, v in documents['#pol_sents_wt_name'].items()])\n",
    "    else:pos_sents_wt_name, neg_sents_wt_name = 0, 0\n",
    "    if documents['#pol_sents_wo_name']:\n",
    "        pos_sents_wo_name = max([v if k == 'Positive' else 0 for k, v in documents['#pol_sents_wo_name'].items()])\n",
    "        neg_sents_wo_name = max([v if k == 'Negative' else 0 for k, v in documents['#pol_sents_wo_name'].items()])\n",
    "    else:pos_sents_wo_name, neg_sents_wo_name = 0, 0\n",
    "    neutral_score = round((documents['#sents_no_model'] / documents['#sents_actual']) * 0.25 , 3)\n",
    "    pos_score = round((pos_sents_wt_name / documents['#sents_actual'] * 1 * 1) + (pos_sents_wo_name / documents['#sents_actual'] * 1), 3)\n",
    "    neg_score = round((neg_sents_wt_name / documents['#sents_actual'] * 1 * 1.5) + (neg_sents_wo_name / documents['#sents_actual'] * 1.5), 3)\n",
    "    scores['Neutral'] = neutral_score\n",
    "    scores['Positive'] = pos_score\n",
    "    scores['Negative'] = neg_score\n",
    "    documents['scores'] = scores  \n",
    "    documents['_doc_polarity'] = max(documents['scores'], key = documents['scores'].get)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Inference Config**\n",
    "\n",
    "Configuration file used in Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_config = InferenceConfig(entry_script = 'scoring.py', environment = myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attach AKS Cluster as Target**\n",
    "\n",
    "We will attach an existing cluster that is created as our compute target for the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_name = 'coe-aks-cluster'\n",
    "if aks_name in ws.compute_targets:\n",
    "    aks_target = ws.compute_targets[aks_name]\n",
    "\n",
    "print(aks_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploy Model as Web Service in AKS Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration \n",
    "aks_config = AksWebservice.deploy_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service_name = \"absa-sentiment-predictor-v2\" # webservice name\n",
    "aks_service = Model.deploy(workspace = ws, \n",
    "                          name = aks_service_name,\n",
    "                          models = [aspect_lex, opinion_lex],\n",
    "                          inference_config = inf_config,\n",
    "                          deployment_target = aks_target, \n",
    "                          overwrite = True)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs \n",
    "aks_service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the web service**\n",
    "\n",
    "Here we test our deployed model via **run method** with a sample input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "test_sent = '{\"news\": \"Tata Motors (NYSE:TTM) is up 7.05% is after the company updated on global wholesales earlier in the day. Group global wholesales including Jaguar Land Rover fell 5% in March to 145,459 units. Global wholesales of all Tata Motors commercial vehicles and Tata Daewoo range were up 1%. Jaguar wholesales were 20,985 units during the month and Land Rover wholesales were 49,186 units.\",\"name\": \"Tata Motors\"}'\n",
    "test_sample = bytes(test_sent, encoding = 'utf-8')\n",
    "prediction = aks_service.run(input_data = test_sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Scoring via HTTP REQUEST**\n",
    "- Here we test our deployed model via **HTTP REQUEST** with a sample input\n",
    "- Sample input text takes two key-value pairs string namely news (a.k.a. new content) and name (a.k.a. company name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk import flatten\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = '1Q7d5p2SqViNlQbhe6gtHBAiZ5MB58rU' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_2 = '{\"news\":\"New Delhi, Apr 8 (PTI) Swiss power major ABB India on Wednesday said it has postponed its annual general meeting, scheduled on May 14, till August in view of the lockdown to contain COVID-19. considering the situation of complete lock down due to COVID-19 in India and the uncertainty of situation returning to normalcy, the Board of Directors of the Company by a resolution passed by circulation, yesterday, has decided to postpone the AGM, which was scheduled on May 14, 2020, cancel the book closure dates and cut-off date accordingly, a regulatory filing said. According to the filing, the Board has also authorized the company to make an application to the Registrar of Companies. Karnataka seeking extension of time up to August 31, 2020 for holding the AGM for the Financial Year 2019. This is with reference to our earlier announcement dated February 12, 2020 intimating the date of 70th Annual General Meeting (AGM) of the company, recommendation of dividend by the Board, dates for book closure and cut-off date for ascertaining list of eligible members for payment of dividend, if declared at the AGM, it added. On receipt of approval from the Registrar of Companies extending the time for holding the AGM, the Board will fix the revised date of AGM, book closure dates and cut-off date. The company will intimate the revised dates to the stock exchanges as soon as it is finalised. PTI KKS BAL BAL\",\"name\":\"ABB India Ltd\"}'\n",
    "test_sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obtain headers\n",
    "headers = {'Content-Type':'application/json', 'Authorization':'Bearer ' + key1}\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\"http://52.187.124.32:80/api/v1/service/absa-sentiment-predictor-v2/score\", \n",
    "                         test_sample_2, \n",
    "                         headers = headers)\n",
    "response = response.json()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
