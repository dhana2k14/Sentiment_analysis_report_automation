{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ABSA Model Training with Azure Machine Learning\n",
    "\n",
    "- This Notebook illustrates how to train a Aspect Based Sentiment Analysis (ABSA) model using Intel's NLP Architect library\n",
    "- This model is trained with Azure Machine Learning Workspace (AML)\n",
    "- For detailed study refer to the [Repository](https://github.com/microsoft/nlp-recipes/tree/master/examples/sentiment_analysis/absa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Azure Core libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core \n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import RemoteCompute, ComputeTarget\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.train.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(azureml.core.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with proxy (optional)\n",
    "# import os \n",
    "# proxy = \"https://<username>:<password>@<proxy-ip>:<port>\"\n",
    "# os.environ['http_proxy'] = proxy\n",
    "# os.environ['https_proxy'] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize AML Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = Workspace.from_config('../../config/config.json')\n",
    "    print(ws.name, ws.location, ws.resource_group, sep = '\\t')\n",
    "    print('Initializing Workspace succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Training Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 'train.py'\n",
    "import argparse\n",
    "import json\n",
    "import os, re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from nltk import flatten\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import NLP architect\n",
    "from nlp_architect.models.absa.train.train import TrainSentiment\n",
    "from nlp_architect.models.absa.inference.inference import SentimentInference\n",
    "\n",
    "#inputs \n",
    "parser = argparse.ArgumentParser(description='ABSA Train')\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--asp_thresh', type=int, default=3)\n",
    "parser.add_argument('--op_thresh', type=int, default=2)\n",
    "parser.add_argument('--max_iter', type=int, default=3)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "from spacy.cli.download import download as spacy_download\n",
    "from nlp_architect.utils.io import uncompress_file\n",
    "from nlp_architect.models.absa import TRAIN_OUT\n",
    "\n",
    "spacy_download('en')\n",
    "GLOVE_ZIP = os.path.join(args.data_folder, 'news_data/glove.840B.300d.zip')\n",
    "EMBEDDING_PATH = TRAIN_OUT / 'word_emb_unzipped' / 'glove.840B.300d.txt'\n",
    "print(\"Embedding Data File Path...\")\n",
    "print(EMBEDDING_PATH)\n",
    "\n",
    "uncompress_file(GLOVE_ZIP, Path(EMBEDDING_PATH).parent)\n",
    "\n",
    "news_train = os.path.join(args.data_folder,'news_data/all_news_content_lang_en.txt.csv') # change here for input dataset\n",
    "print(\"Input Dataset Location...\")\n",
    "print(news_train)\n",
    "\n",
    "aspect_path = TRAIN_OUT / 'lexicons' / 'generated_aspect_lex.csv'\n",
    "opinion_path = TRAIN_OUT / 'lexicons' / 'generated_opinion_lex_reranked.csv'\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "train = TrainSentiment(asp_thresh=args.asp_thresh, op_thresh=args.op_thresh, max_iter=args.max_iter)\n",
    "\n",
    "try:\n",
    "    opinion_lex, aspect_lex = train.run(data=news_train, out_dir = './outputs')\n",
    "except:\n",
    "    print(\"ValueError!\")\n",
    "    \n",
    "# helper function \n",
    "def word_freq(word_list):\n",
    "    \"\"\"\n",
    "    Return Polarity    \n",
    "    \"\"\"\n",
    "    word_freq = [word_list.count(w) for w in word_list]\n",
    "    return(dict(zip(word_list, word_freq)))\n",
    "\n",
    "# helper function\n",
    "def doc2label(doc):\n",
    "    \"\"\"\n",
    "    Converts ABSA Inference Doc to Sentiment Labels  \n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    sentences_list = []\n",
    "    line_json = json.loads(doc.json())\n",
    "    text = line_json['_doc_text']\n",
    "    sents = line_json['_sentences']\n",
    "    events = []\n",
    "    for i in range(len(sents)):\n",
    "        for e in sents[i]['_events']:\n",
    "            for ev in e:\n",
    "                if ev['_type'] == 'OPINION':\n",
    "                     events.append(ev)\n",
    "    events = {d['_text']:d for d in events}.values() # get unique events\n",
    "    label_list = [e['_polarity'] for e in list(events)] # extract polarity for count\n",
    "    if label_list:\n",
    "        if max(word_freq(label_list), key = word_freq(label_list).get) == 'POS':doc_sentiment = 'Positive'\n",
    "        else:doc_sentiment = 'Negative'\n",
    "    tokens = text.split()\n",
    "    io = [[token, 'O'] for token in tokens]\n",
    "    index = 0\n",
    "    for token_id, token in enumerate(tokens):\n",
    "        for event in events:\n",
    "            if event['_start'] == index:\n",
    "                io[token_id][1] = \"<{}>\".format(event['_polarity'])\n",
    "        index += len(token) + 1\n",
    "    io = flatten(io)\n",
    "    while 'O' in io:\n",
    "        io.remove('O')\n",
    "    output = \" \".join([l for l in io])\n",
    "    for id, sent in enumerate(sents):\n",
    "        sent_polarity = {}\n",
    "        s = text[sent['_start']:sent['_end'] + 1]\n",
    "        s_tokens = s.split()\n",
    "        s_io = [[tok, 'O'] for tok in s_tokens]\n",
    "        for tok_id, tok in enumerate(s_tokens):\n",
    "            for event in events:\n",
    "                if event['_text'] == tok:\n",
    "                    s_io[tok_id][1] = \"<{}>\".format(event['_polarity'])\n",
    "        s_io = flatten(s_io)\n",
    "        while 'O' in s_io:\n",
    "            s_io.remove('O')\n",
    "        sentence = \" \".join([l for l in s_io])\n",
    "        polarity = re.findall(r'<NEG>|<POS>', sentence)\n",
    "        polarity = [re.sub(r'<|>', '', i) for i in polarity]\n",
    "        if polarity:\n",
    "            if max(word_freq(polarity), key = word_freq(polarity).get) == 'POS':polarity = 'Positive'\n",
    "            else:polarity = 'Negative'\n",
    "        sent_polarity['sentence ' + str(id + 1)] = sentence # sentence dict\n",
    "        sent_polarity['polarity'] = polarity\n",
    "        sentences_list.append(sent_polarity)\n",
    "    documents['_news_text'] = text\n",
    "    documents['_doc_polarity'] = doc_sentiment\n",
    "    documents['_sentences'] = sentences_list\n",
    "    return documents\n",
    "\n",
    "inference = SentimentInference(aspect_path, opinion_path)\n",
    "shutil.copyfile(aspect_path, './outputs/news_content_aspect.csv')\n",
    "shutil.copyfile(opinion_path, './outputs/news_content_opinion.csv')\n",
    "  \n",
    "input_file_path = os.path.join(args.data_folder,'news_data/all_news_content.csv')\n",
    "print(f'Aspect and Opinion lexicons files loaded from {aspect_path} and {opinion_path}')\n",
    "print(f'Input file loaded from {input_file_path}')\n",
    "\n",
    "inference = SentimentInference(aspect_path, opinion_path)\n",
    "\n",
    "# Get Inference Results               \n",
    "with open(input_file_path, 'r') as csv_file:\n",
    "    lines = csv_file.readlines()\n",
    "    for id, line in enumerate(lines):\n",
    "        if line:\n",
    "            sentiment_doc = inference.run(doc = line)\n",
    "            if sentiment_doc != None:\n",
    "                labels = doc2label(sentiment_doc)\n",
    "                labels['sent_id'] = id + 1\n",
    "                with open('./outputs/' + 'sentiment_labels_v1.json', 'a') as json_file:\n",
    "                    json_file.write(json.dumps(labels))\n",
    "                    json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input CPU or GPU cluster compute target name\n",
    "# Compute Target should be created before running this script\n",
    "compute_name = 'coe-gpu-cluster' \n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "\n",
    "print(compute_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify experiment name and define experiment object \n",
    "experiment_name = 'absa-GPU'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training script location\n",
    "# location for training data for models and other parameters \n",
    "script_params = {'--data_folder': ds.as_download()}\n",
    "nlp_est = Estimator(source_directory='../',\n",
    "                   script_params=script_params,\n",
    "                   compute_target=compute_target,\n",
    "                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n",
    "                   entry_script='train.py',\n",
    "                   pip_packages= ['nlp_architect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit Model Training**\n",
    "- Model Training can be submitted as a Job with AML which can be tracked using **Experiments** Tab in Azure Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = exp.submit(nlp_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize you model training run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Outputs to Local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model output files to your local project folder from run instance \n",
    "run.download_file(name = 'outputs/news_content_aspect.csv', output_file_path = '../output/news_content_aspect_gpu.csv')\n",
    "run.download_file(name = 'outputs/news_content_opinion.csv', output_file_path = '../output/news_content_opinion_gpu.csv')\n",
    "run.download_file(name = 'outputs/sentiment_labels_v1.json', output_file_path = '../output/sentiment_labels_10k_v1.json')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
