{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Classification Model Training\n",
    "- This notebook explains how to train a sentence classification model using Machine Learning Algorithms \n",
    "- We use **Universal Sentence Encoder** pre-trained language model from **Tensorflow Hub** to obtain text representations for our text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQU8mbDoUFVl"
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WIP72GlKSJc",
    "outputId": "b5be4347-d973-49c2-d86b-fab3075c2d0b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "RFjBTWBavYNU",
    "outputId": "e933dc19-aa6b-4e62-ff07-1dca60868e74"
   },
   "outputs": [],
   "source": [
    "# read training data from drive \n",
    "ds1 = pd.read_excel('../../input/data_Categorization_set1.xlsx', sheet_name = 1, usecols = ['News', 'Sub-Categories'])\n",
    "ds2 = pd.read_excel('../../input/data_Categorization_set2.xlsx', sheet_name = 1, usecols = ['News', 'Sub-Categories'])\n",
    "ds1 = ds1[ds1['Sub-Categories'].notnull()]\n",
    "ds2 = ds2[ds2['Sub-Categories'].notnull()]\n",
    "ds = pd.concat([ds1, ds2], axis = 0)\n",
    "ds.drop_duplicates('News', keep = 'first', inplace = True)\n",
    "list_sentences = ds['News'].tolist()\n",
    "print(len(list_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sldITlmdxamB"
   },
   "outputs": [],
   "source": [
    "# load pre-trained language model from Tfhub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m69Il-eW6OVR"
   },
   "outputs": [],
   "source": [
    "# helper function \n",
    "def embed_text(text):\n",
    "    embeddings = embed(text)\n",
    "    return[vector.numpy().tolist() for vector in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opg1uq1q5qA-"
   },
   "outputs": [],
   "source": [
    "# get embeddings for all sentences \n",
    "list_of_sentence_vectors = embed_text(list_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo9FJK9V7FHd"
   },
   "outputs": [],
   "source": [
    "# convert list of sentence vectors into a dataframe\n",
    "embeddings_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(list_of_sentence_vectors)):\n",
    "    df = pd.DataFrame([list_of_sentence_vectors[i]])\n",
    "    embeddings_df = embeddings_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XvrnvRrLW3U"
   },
   "outputs": [],
   "source": [
    "# save the embeddings in a file\n",
    "embeddings_df.to_csv('../../output/sentence_embeddings_tfh.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szel6BHXNhlW"
   },
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubA9kyNuN4Er",
    "outputId": "003f02d2-27aa-48dd-ee6b-04212aff983b"
   },
   "outputs": [],
   "source": [
    "# import libraries \n",
    "from pprint import pprint\n",
    "import logging\n",
    "from time import time\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGOU6ktEkF-d",
    "outputId": "5836bb45-4091-446e-8932-dedeed25c52f"
   },
   "outputs": [],
   "source": [
    "# read embeddings that is saved in disc\n",
    "embeddings_df = pd.read_csv(data_path + 'sentence_embeddings_tfh.csv')\n",
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "rfjk48hFMjtk",
    "outputId": "f86b4558-27ea-408d-f1be-09f65e33fb7c"
   },
   "outputs": [],
   "source": [
    "# specify Xs and Ys\n",
    "X = embeddings_df\n",
    "y = ds['Sub-Categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "kGwpXSkyDfVP",
    "outputId": "3c8309e4-fcd4-4f31-90fd-efac0a312c48"
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 999)\n",
    "print(f'Training data size: {train_x.shape}')\n",
    "print(f'Testing data size: {test_x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QCgu-Rx0SQi"
   },
   "outputs": [],
   "source": [
    "# create pipeline for various models to train\n",
    "logit_ppl = Pipeline([('logit', LogisticRegression(multi_class='ovr', solver='liblinear'))])\n",
    "rf_ppl = Pipeline([('rf', RandomForestClassifier(n_estimators = 50))])\n",
    "xgb_ppl = Pipeline([('xgb', xgb.XGBClassifier(objective = 'multi:softmax', ))])\n",
    "svm_ppl = Pipeline([('svm', LinearSVC(multi_class = 'ovr'))])\n",
    "\n",
    "# parameters for Gridsearch\n",
    "param_logitGv = {'logit__max_iter':[100, 500, 1000]}\n",
    "param_rfGv = {'rf__min_samples_split': [10, 20, 50]}\n",
    "param_svmGv = {'svm__max_iter': [100, 500, 1000]}\n",
    "param_xgbGv = {'xgb__learning_rate': [.01, .05], 'xgb__n_estimators': [10, 50]}\n",
    "\n",
    "logitGv = GridSearchCV(logit_ppl, param_logitGv, cv = 5)\n",
    "rfGv = GridSearchCV(rf_ppl, param_rfGv, cv = 5)\n",
    "svmGv = GridSearchCV(svm_ppl, param_svmGv, cv = 5)\n",
    "xgbGv = GridSearchCV(xgb_ppl, param_xgbGv, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jK-r6cfcGvAU",
    "outputId": "cf47adf9-9c43-4952-cae3-43070fd1efc0"
   },
   "outputs": [],
   "source": [
    "# train models \n",
    "print('Performing Grid Search ...')\n",
    "print('Pipeline:', [name for name, _ in logit_ppl.steps])\n",
    "print('Parameters:')\n",
    "pprint(param_logitGv)\n",
    "t0 = time()\n",
    "logitGv.fit(train_x, train_y)\n",
    "print('Done in %0.3fs' % (time() - t0))\n",
    "print()\n",
    "\n",
    "print('Best score %0.3f' % logitGv.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = logitGv.best_estimator_.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('\\t%s:%r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print('-----' * 30)\n",
    "\n",
    "print('Performing Grid Search ...')\n",
    "print('Pipeline:', [name for name, _ in rf_ppl.steps])\n",
    "print('Parameters:')\n",
    "pprint(param_rfGv)\n",
    "to = time()\n",
    "rfGv.fit(train_x, train_y)\n",
    "print('Done in %0.3fs' % (time() - t0))\n",
    "print()\n",
    "\n",
    "print('Best score %0.3f' % rfGv.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = rfGv.best_estimator_.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('\\t%s:%r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print('-----' * 30)\n",
    "\n",
    "print('Performing Grid Search ...')\n",
    "print('Pipeline:', [name for name, _ in svm_ppl.steps])\n",
    "print('Parameters:')\n",
    "pprint(param_svmGv)\n",
    "to = time()\n",
    "svmGv.fit(train_x, train_y)\n",
    "print('Done in %0.3fs' % (time() - t0))\n",
    "print()\n",
    "\n",
    "print('Best score %0.3f' % svmGv.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = svmGv.best_estimator_.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('\\t%s:%r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print('-----' * 30)\n",
    "\n",
    "print('Performing Grid Search ...')\n",
    "print('Pipeline:', [name for name, _ in xgb_ppl.steps])\n",
    "print('Parameters:')\n",
    "pprint(param_xgbGv)\n",
    "to = time()\n",
    "xgbGv.fit(train_x, train_y)\n",
    "print('Done in %0.3fs' % (time() - t0))\n",
    "print()\n",
    "\n",
    "print('Best score %0.3f' % xgbGv.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = xgbGv.best_estimator_.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('\\t%s:%r' % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "mZ38uaCAVpBJ",
    "outputId": "a93492b2-7d99-4a22-a801-0e3ed8de7457"
   },
   "outputs": [],
   "source": [
    "# Train a model with best parameters from Grid Search \n",
    "# Predict \n",
    "svm_classifier = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "          intercept_scaling=1, loss='squared_hinge', max_iter=100,\n",
    "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "          verbose=0)\n",
    "svm_model = svm_classifier.fit(train_x, train_y)\n",
    "print(svm_model)\n",
    "print('----' * 10)\n",
    "preds = svm_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "pRiqhbSNW-Ij",
    "outputId": "435f8218-4457-4cab-d070-21c100bafc8b"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "target_labels = train_y.drop_duplicates().values\n",
    "conf_matrix = confusion_matrix(test_y, preds, labels = target_labels)\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "sns.heatmap(conf_matrix, annot = True, fmt = 'd', xticklabels = target_labels, yticklabels = target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcpwNYWQlwm_"
   },
   "source": [
    "**Save & Load Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tagm4Sl-lu2v"
   },
   "outputs": [],
   "source": [
    "joblib.dump(svm_model, '../../input/sent_classifier_model/svm_model_wt_use.pkl') # save model to disc \n",
    "svm_clf_model = joblib.load('../../input/sent_classifier_model/svm_model_wt_use.pkl') # read model from disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc0rLsJ7aTqm"
   },
   "source": [
    "### Sentence Classification Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pe4D1e6PSSxm",
    "outputId": "45a8b2b9-4a0b-4b31-df3e-134efdc1952f"
   },
   "outputs": [],
   "source": [
    "# import dataset with sentences obtained from ABSA models \n",
    "sample_data = pd.read_csv('../../input/sample_data_.csv', encoding = 'latin-1')\n",
    "print(f'Number of Articles: {sample_data.doc_id.value_counts().count()}')\n",
    "print(f'Number of Sentences: {sample_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rft1Z2yTTt29",
    "outputId": "72fd7a36-dd55-4877-8e23-76e662da9ad8"
   },
   "outputs": [],
   "source": [
    "# Create a list of sentences\n",
    "sentence_list = sample_data['sents'].astype('str').tolist()\n",
    "text_vectors = embed_text(sentence_list)\n",
    "len(text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GS7wSX5nNvx"
   },
   "outputs": [],
   "source": [
    "# Generate text representations \n",
    "random_embeddings_df = pd.DataFrame()\n",
    "for i in range(len(text_vectors)):\n",
    "    df = pd.DataFrame([text_vectors[i]])\n",
    "    random_embeddings_df = random_embeddings_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kNVdtgDX2ww"
   },
   "outputs": [],
   "source": [
    "# predict & write results into a text file\n",
    "random_preds = svm_clf_model.predict(random_embeddings_df)\n",
    "for sentence, pred in zip(sentence_list, random_preds):\n",
    "    with open('../../output/results_prediction_for_reports.txt', 'a') as outfile:\n",
    "        results_to_write = sentence + '\\t' + pred\n",
    "        outfile.write(results_to_write)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gfyjUz0YWTK",
    "outputId": "10332657-2870-4d54-a316-e0fdb85b0cc5"
   },
   "outputs": [],
   "source": [
    "# merge model predictions with main dataset\n",
    "results_df = pd.read_table('../../output/results_prediction_for_reports.txt', sep = '\\t', header = None, names = ['sentence', 'category'], usecols = ['category'])\n",
    "sample_data = pd.merge(sample_data[['d_pol', 'doc_id', 'company_name', 'sents', 's_pol']], results_df, how = 'left', left_index=True, right_index=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wWyiO2j2nDXg"
   ],
   "name": "USE_Tensorflowhub.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
